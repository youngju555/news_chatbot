import streamlit as st
import feedparser
import os
import urllib.parse
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from openai import OpenAI
import urllib3

# SSL ê²½ê³  ë¬´ì‹œ (ë°°í¬ í™˜ê²½ ë…¸ì´ì¦ˆ ì œê±°)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# --- ì„¤ì • ë° ì´ˆê¸°í™” ---
API_KEY = os.getenv("OPENAI_API_KEY")
API_BASE = os.getenv("OPENAI_API_BASE")
MODEL_NAME = os.getenv("LLM_MODEL_NAME", "gpt-3.5-turbo")

if API_BASE:
    client = OpenAI(api_key=API_KEY, base_url=API_BASE)
else:
    client = OpenAI(api_key=API_KEY)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="AI ë‰´ìŠ¤ íë ˆì´í„° (Pro)", page_icon="ğŸ“°", layout="wide")
st.title("ğŸ“° AI ì‹¬ì¸µ ë‰´ìŠ¤ ë¶„ì„ ì±—ë´‡")

# --- ì„¸ì…˜ ìƒíƒœ ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "total_tokens" not in st.session_state:
    st.session_state.total_tokens = 0

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("ğŸ“Š ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°")
    st.metric(label="ëˆ„ì  ì‚¬ìš© í† í°", value=f"{st.session_state.total_tokens:,} Tokens")
    if st.button("ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.session_state.total_tokens = 0
        st.rerun()

# --- í•µì‹¬ í•¨ìˆ˜ ---

def calc_tokens(text):
    if not text: return 0
    return int(len(text) * 1.2)

# [ìµœì¢…] BeautifulSoup + ì¿ í‚¤ ì‚¬ìš© (ë°°í¬ ì‹œ ê°€ì¥ ì•ˆì •ì )
def fetch_article_content(url):
    try:
        # 1. í—¤ë” ìœ„ì¥
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.google.com/",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"
        }
        
        # 2. ì¿ í‚¤ ì„¤ì • (êµ¬ê¸€ ë³´ì•ˆ ìš°íšŒ ì‹œë„)
        cookies = {"CONSENT": "YES+cb.20210720-07-p0.en+FX+417"}

        # 3. ìš”ì²­ ë³´ë‚´ê¸° (verify=Falseë¡œ SSL ì—ëŸ¬ ë°©ì§€)
        session = requests.Session()
        response = session.get(url, headers=headers, cookies=cookies, timeout=5, verify=False)
        
        # 4. íŒŒì‹±
        soup = BeautifulSoup(response.text, "html.parser")
        
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup(["script", "style", "header", "footer", "nav", "aside", "form", "iframe", "noscript"]):
            tag.decompose()
            
        # ë³¸ë¬¸ ì¶”ì¶œ
        text_content = ""
        paragraphs = soup.find_all(['p', 'div'])
        for p in paragraphs:
            text = p.get_text().strip()
            if len(text) > 30: 
                text_content += text + " "
        
        if len(text_content) < 200:
            return None
            
        return text_content[:2000] + "..."
        
    except Exception:
        return None

def get_news_data(keyword):
    encoded_keyword = urllib.parse.quote(keyword)
    rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR&ceid=KR:ko"
    
    feed = feedparser.parse(rss_url)
    news_items = []
    
    if not feed.entries:
        return []

    for i, entry in enumerate(feed.entries[:3]):
        content = entry.get('summary', 'ìš”ì•½ ì—†ìŒ')
        source_type = "Google RSS ìš”ì•½"
        
        # ìƒìœ„ 2ê°œ í¬ë¡¤ë§ ì‹œë„
        if i < 2:
            crawled_text = fetch_article_content(entry.link)
            if crawled_text:
                content = crawled_text
                source_type = "ğŸŒ ì›¹ì‚¬ì´íŠ¸ ë³¸ë¬¸ (ë¶„ì„ ì„±ê³µ)"
        
        news_items.append({
            "title": entry.title,
            "link": entry.link,
            "content": content,
            "source_type": source_type
        })
    
    return news_items

def classify_intent(user_input):
    system_prompt = "ì‚¬ìš©ìê°€ ë‰´ìŠ¤ ê²€ìƒ‰ì„ ì›í•˜ë©´ 'SEARCH:í‚¤ì›Œë“œ', ì•„ë‹ˆë©´ 'CHAT'ì´ë¼ê³  ë‹µí•˜ì„¸ìš”."
    input_tokens = calc_tokens(system_prompt + user_input)

    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_input}]
        )
        content = response.choices[0].message.content.strip()
        st.session_state.total_tokens += (input_tokens + calc_tokens(content))

        if content.startswith("SEARCH:"):
            return {"type": "SEARCH", "keyword": content.split("SEARCH:")[1].strip()}
        return {"type": "CHAT", "keyword": None}
    except:
        return {"type": "CHAT", "keyword": None}

def get_llm_response(prompt, context_type="general", news_data=None):
    messages = st.session_state.messages.copy()
    
    if context_type == "news_summary" and news_data:
        news_context = ""
        for idx, item in enumerate(news_data, 1):
            news_context += f"[{idx}] {item['title']} ({item['source_type']})\në§í¬: {item['link']}\në‚´ìš©: {item['content']}\n\n"
            
        system_prompt = "ë‹¹ì‹ ì€ ì‹¬ì¸µ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. [ë‰´ìŠ¤ ë°ì´í„°]ë¥¼ ë³´ê³  ë‹µë³€í•˜ì„¸ìš”. 'ì›¹ì‚¬ì´íŠ¸ ë³¸ë¬¸'ì´ ìˆë‹¤ë©´ ê·¸ ë‚´ìš©ì„ ì ê·¹ ì¸ìš©í•˜ì„¸ìš”."
        user_content = f"ì§ˆë¬¸: {prompt}\n\n[ë‰´ìŠ¤ ë°ì´í„°]\n{news_context}"
        
        messages_for_api = [{"role": "system", "content": system_prompt}] + messages
        messages_for_api.append({"role": "user", "content": user_content})
    else:
        messages_for_api = [{"role": "system", "content": "ë„ì›€ì´ ë˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤."}] + messages
        messages_for_api.append({"role": "user", "content": prompt})

    all_text = "".join([str(m["content"]) for m in messages_for_api])
    input_tokens = calc_tokens(all_text)

    try:
        stream = client.chat.completions.create(model=MODEL_NAME, messages=messages_for_api, stream=True)
        return stream, input_tokens
    except Exception as e:
        return f"ì—ëŸ¬: {e}", 0

# --- ë©”ì¸ ë¡œì§ ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        intent_result = classify_intent(prompt)
        stream_response = None
        input_tokens = 0
        
        if intent_result["type"] == "SEARCH":
            keyword = intent_result["keyword"]
            with st.status(f"ğŸ•µï¸ '{keyword}' ë¶„ì„ ì¤‘...", expanded=True) as status:
                news_items = get_news_data(keyword)
                
                success = sum(1 for item in news_items if "ë¶„ì„ ì„±ê³µ" in item['source_type'])
                if success > 0:
                    status.update(label=f"âœ… {success}ê±´ ì›ë¬¸ ë¶„ì„ ì„±ê³µ!", state="complete", expanded=False)
                else:
                    status.update(label="âš ï¸ ìš”ì•½ë³¸ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤ (ë³´ì•ˆ ì ‘ì† ë¶ˆê°€)", state="complete", expanded=False)
                
                stream_response, input_tokens = get_llm_response(prompt, "news_summary", news_items)
        else:
            stream_response, input_tokens = get_llm_response(prompt, "general")

        if stream_response and not isinstance(stream_response, str):
            for chunk in stream_response:
                if chunk.choices[0].delta.content:
                    full_response += chunk.choices[0].delta.content
                    message_placeholder.markdown(full_response + "â–Œ")
            
            st.session_state.total_tokens += (input_tokens + calc_tokens(full_response))
            message_placeholder.markdown(full_response)
        else:
            message_placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})