# import streamlit as st
# import feedparser
# import os
# import urllib.parse
# import requests  # ì›¹ ìš”ì²­ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
# from bs4 import BeautifulSoup  # HTML ë¶„ì„ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
# from dotenv import load_dotenv
# from openai import OpenAI

# # 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# load_dotenv()

# # --- ì„¤ì • ë° ì´ˆê¸°í™” ---
# API_KEY = os.getenv("OPENAI_API_KEY")
# API_BASE = os.getenv("OPENAI_API_BASE")
# MODEL_NAME = os.getenv("LLM_MODEL_NAME", "gpt-3.5-turbo")

# # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
# if API_BASE:
#     client = OpenAI(api_key=API_KEY, base_url=API_BASE)
# else:
#     client = OpenAI(api_key=API_KEY)

# # í˜ì´ì§€ ì„¤ì •
# st.set_page_config(page_title="AI ë‰´ìŠ¤ íë ˆì´í„° (Pro)", page_icon="ğŸ“°", layout="wide")
# st.title("ğŸ“° AI ì‹¬ì¸µ ë‰´ìŠ¤ ë¶„ì„ ì±—ë´‡")

# # --- ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# # [ìœ ì§€ë¨] í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì  ê¸°ë¡
# if "total_tokens" not in st.session_state:
#     st.session_state.total_tokens = 0

# # --- ì‚¬ì´ë“œë°”: ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„° (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€) ---
# with st.sidebar:
#     st.header("ğŸ“Š ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°")
#     st.info("APIì—ì„œëŠ” 'ë‚¨ì€ ì”ì•¡'ì„ ì•Œë ¤ì£¼ì§€ ì•Šì•„ì„œ, 'ì´ë²ˆ ëŒ€í™” ì‚¬ìš©ëŸ‰'ì„ ì¶”ì‚°í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤.")
    
#     # ë¯¸í„°ê¸° í‘œì‹œ
#     st.metric(label="ëˆ„ì  ì‚¬ìš© í† í° (ì¶”ì •)", value=f"{st.session_state.total_tokens:,} Tokens")
    
#     # íŒ
#     st.caption(f"í˜„ì¬ ëª¨ë¸: {MODEL_NAME}")
#     st.caption("â€» í•œê¸€ 1ê¸€ì â‰ˆ 1~2í† í°")
    
#     if st.button("ëŒ€í™” & ì‚¬ìš©ëŸ‰ ì´ˆê¸°í™”"):
#         st.session_state.messages = []
#         st.session_state.total_tokens = 0
#         st.rerun()

# # --- í•µì‹¬ í•¨ìˆ˜ ì •ì˜ ---

# def calc_tokens(text):
#     """í† í° ìˆ˜ë¥¼ ëŒ€ëµì ìœ¼ë¡œ ê³„ì‚° (ê¸€ì ìˆ˜ * 1.2)"""
#     if not text:
#         return 0
#     return int(len(text) * 1.2)

# # [ìƒˆë¡œ ì¶”ê°€ë¨] ì›¹í˜ì´ì§€ ë³¸ë¬¸ ê¸ì–´ì˜¤ê¸° (BeautifulSoup ì‚¬ìš©)
# # [ìˆ˜ì •ë¨] ë” ê°•ë ¥í•œ í¬ë¡¤ë§ í•¨ìˆ˜
# def fetch_article_content(url):
#     try:
#         # 1. ê°•ë ¥í•œ ìœ„ì¥ (í¬ë¡¬ ë¸Œë¼ìš°ì €ì¸ ì²™ í—¤ë” ì„¤ì •)
#         headers = {
#             "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
#             "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
#             "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
#             "Referer": "https://www.google.com/"
#         }
        
#         # 2. ì„¸ì…˜ ì‚¬ìš© (ì ‘ì† ìœ ì§€ë ¥ í–¥ìƒ)
#         session = requests.Session()
        
#         # 3. ì ‘ì† ì‹œë„ (íƒ€ì„ì•„ì›ƒ 3ì´ˆ -> 15ì´ˆë¡œ ëŠ˜ë¦¼)
#         response = session.get(url, headers=headers, timeout=15, allow_redirects=True)
#         response.raise_for_status() # 404, 403 ì—ëŸ¬ ì²´í¬
        
#         # 4. ì¸ì½”ë”© ìë™ ê°ì§€ (í•œê¸€ ê¹¨ì§ ë°©ì§€)
#         response.encoding = response.apparent_encoding

#         # 5. HTML íŒŒì‹±
#         soup = BeautifulSoup(response.text, "html.parser")
        
#         # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
#         for tag in soup(["script", "style", "header", "footer", "nav", "aside", "form", "iframe"]):
#             tag.decompose()
            
#         # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (p íƒœê·¸ë¿ë§Œ ì•„ë‹ˆë¼ divì˜ ë³¸ë¬¸ë„ ê³ ë ¤)
#         text_content = " ".join([p.get_text().strip() for p in soup.find_all(['p', 'div'])])
        
#         # ê³µë°± ì •ë¦¬
#         import re
#         text_content = re.sub(r'\s+', ' ', text_content).strip()

#         # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´(300ì ë¯¸ë§Œ) ì‹¤íŒ¨ë¡œ ê°„ì£¼ (ë©”ë‰´íŒë§Œ ê¸ì–´ì˜¤ëŠ” ê²½ìš° ë°©ì§€)
#         if len(text_content) < 300:
#             print(f"ë‚´ìš© ë¶€ì¡± ({len(text_content)}ì): {url}") # í„°ë¯¸ë„ ë””ë²„ê¹…ìš©
#             return None
            
#         return text_content[:2000] + "..." # 2000ì ì œí•œ
        
#     except Exception as e:
#         print(f"í¬ë¡¤ë§ ì—ëŸ¬ ë°œìƒ: {e}") # í„°ë¯¸ë„ì—ì„œ ì—ëŸ¬ ë‚´ìš© í™•ì¸ ê°€ëŠ¥
#         return None
    
# def get_news_data(keyword):
#     encoded_keyword = urllib.parse.quote(keyword)
#     rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR&ceid=KR:ko"
    
#     feed = feedparser.parse(rss_url)
#     news_items = []
    
#     if not feed.entries:
#         return []

#     # ìƒìœ„ 3ê°œ ê¸°ì‚¬ë§Œ ì²˜ë¦¬
#     for i, entry in enumerate(feed.entries[:3]):
#         content = entry.get('summary', 'ìš”ì•½ ì—†ìŒ')
#         source_type = "Google RSS ìš”ì•½" # ê¸°ë³¸ê°’
        
#         # [ì—…ê·¸ë ˆì´ë“œ] ìƒìœ„ 2ê°œëŠ” ì§ì ‘ ì ‘ì†í•´ì„œ ë³¸ë¬¸ ì½ê¸° ì‹œë„
#         if i < 2:
#             # ì‚¬ì´ë“œë°”ì— ë¡œë”© ìƒíƒœ í‘œì‹œí•˜ì§€ ì•Šê³  ì¡°ìš©íˆ ì²˜ë¦¬í•˜ê±°ë‚˜, 
#             # í•„ìš”í•˜ë©´ st.toast ë“±ìœ¼ë¡œ ì•Œë¦´ ìˆ˜ ìˆìŒ. ì—¬ê¸°ì„  ì†ë„ë¥¼ ìœ„í•´ ìƒëµ.
#             crawled_text = fetch_article_content(entry.link)
#             if crawled_text:
#                 content = crawled_text
#                 source_type = "ğŸŒ ì›¹ì‚¬ì´íŠ¸ ë³¸ë¬¸ (í¬ë¡¤ë§ë¨)"
        
#         news_items.append({
#             "title": entry.title,
#             "link": entry.link,
#             "content": content,
#             "source_type": source_type
#         })
    
#     return news_items

# def classify_intent(user_input):
#     system_prompt = """
#     ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” AIì…ë‹ˆë‹¤.
#     ì‚¬ìš©ìì˜ ì…ë ¥ì´ 'ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ìµœê·¼ ì†Œì‹' ë“±ì„ ë¬»ëŠ” ê²€ìƒ‰ ìš”ì²­ì´ë¼ë©´ 'SEARCH:ê²€ìƒ‰í‚¤ì›Œë“œ' í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
#     ê²€ìƒ‰ ìš”ì²­ì´ ì•„ë‹ˆë¼ë©´ 'CHAT'ì´ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
#     """
    
#     input_tokens = calc_tokens(system_prompt + user_input)

#     try:
#         response = client.chat.completions.create(
#             model=MODEL_NAME,
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": user_input}
#             ]
#         )
#         content = response.choices[0].message.content.strip()
        
#         output_tokens = calc_tokens(content)
#         st.session_state.total_tokens += (input_tokens + output_tokens) # ë¶„ë¥˜ í† í° ëˆ„ì 

#         if content.startswith("SEARCH:"):
#             keyword = content.split("SEARCH:")[1].strip()
#             return {"type": "SEARCH", "keyword": keyword}
#         else:
#             return {"type": "CHAT", "keyword": None}
            
#     except Exception as e:
#         return {"type": "CHAT", "keyword": None}

# def get_llm_response(prompt, context_type="general", news_data=None):
#     messages = st.session_state.messages.copy()
    
#     system_prompt = ""
#     user_content = prompt

#     if context_type == "news_summary" and news_data:
#         # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ LLMì´ ì½ê¸° í¸í•˜ê²Œ í¬ë§·íŒ…
#         news_context = ""
#         for idx, item in enumerate(news_data, 1):
#             news_context += f"""
#             [{idx}] ì œëª©: {item['title']}
#             - ì¶œì²˜ìœ í˜•: {item['source_type']}
#             - ë§í¬: {item['link']}
#             - ë‚´ìš©: {item['content']}
#             \n"""
            
#         system_prompt = f"""
#         ë‹¹ì‹ ì€ ì‹¬ì¸µ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. [ê²€ìƒ‰ëœ ë‰´ìŠ¤ ë°ì´í„°]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
#         'ì›¹ì‚¬ì´íŠ¸ ë³¸ë¬¸'ì´ í¬í•¨ëœ ê²½ìš°, ê·¸ ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
#         ë°˜ë“œì‹œ ê° ê¸°ì‚¬ì˜ ì¶œì²˜ ë§í¬ë¥¼ í¬í•¨í•˜ì„¸ìš”.
#         """
#         user_content = f"ì‚¬ìš©ì ì§ˆë¬¸: {prompt}\n\n[ê²€ìƒ‰ëœ ë‰´ìŠ¤ ë°ì´í„°]\n{news_context}"
#     else:
#         system_prompt = "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤."

#     # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€)
#     messages_for_api = [{"role": "system", "content": system_prompt}] + messages
    
#     if context_type == "news_summary":
#         # ë§ˆì§€ë§‰ ë©”ì‹œì§€ë¥¼ ë‰´ìŠ¤ ë°ì´í„°ê°€ í¬í•¨ëœ ë²„ì „ìœ¼ë¡œ êµì²´ (API ì „ì†¡ìš©)
#         messages_for_api.append({"role": "user", "content": user_content})

#     # ì…ë ¥ í† í° ê³„ì‚°
#     all_text = "".join([str(m["content"]) for m in messages_for_api])
#     input_tokens = calc_tokens(all_text)

#     try:
#         stream = client.chat.completions.create(
#             model=MODEL_NAME,
#             messages=messages_for_api,
#             stream=True
#         )
#         return stream, input_tokens
#     except Exception as e:
#         return f"API í˜¸ì¶œ ì˜¤ë¥˜ ë°œìƒ: {e}", 0

# # --- UI ë° ë©”ì¸ ë¡œì§ ---

# # 1. ëŒ€í™” ê¸°ë¡ í‘œì‹œ
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.markdown(message["content"])

# # 2. ì‚¬ìš©ì ì…ë ¥
# if prompt := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     with st.chat_message("assistant"):
#         message_placeholder = st.empty()
#         full_response = ""
        
#         # A. ì˜ë„ íŒë‹¨
#         intent_result = classify_intent(prompt)
        
#         stream_response = None
#         input_tokens_estimate = 0
        
#         # B. ë¶„ê¸° ì²˜ë¦¬
#         if intent_result["type"] == "SEARCH":
#             keyword = intent_result["keyword"]
#             with st.status(f"ğŸ•µï¸ '{keyword}' ì‹¬ì¸µ ë¶„ì„ ì¤‘...", expanded=True) as status:
#                 st.write("1. êµ¬ê¸€ ë‰´ìŠ¤ RSS ìˆ˜ì§‘ ì¤‘...")
#                 news_items = get_news_data(keyword)
                
#                 # í¬ë¡¤ë§ ì„±ê³µ ì—¬ë¶€ ì²´í¬
#                 crawled_count = sum(1 for item in news_items if "í¬ë¡¤ë§" in item['source_type'])
#                 if crawled_count > 0:
#                     status.update(label=f"ì›¹ì‚¬ì´íŠ¸ {crawled_count}ê³³ ì›ë¬¸ í™•ë³´ ì™„ë£Œ! ë¶„ì„ ì¤‘...", state="running")
#                 else:
#                     status.update(label="ì›ë¬¸ ì ‘ì† ë¶ˆê°€. ìš”ì•½ë³¸ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.", state="running")
                
#                 if news_items:
#                     stream_response, input_tokens_estimate = get_llm_response(prompt, "news_summary", news_items)
#                     status.update(label="ì™„ë£Œ!", state="complete", expanded=False)
#                 else:
#                     status.update(label="ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ", state="error")
#                     full_response = "ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

#         else: # ì¼ë°˜ ëŒ€í™”
#             stream_response, input_tokens_estimate = get_llm_response(prompt, "general")

#         # C. ì‘ë‹µ ì¶œë ¥ ë° í† í° ì •ì‚°
#         if stream_response:
#             if not isinstance(stream_response, str):
#                 for chunk in stream_response:
#                     if chunk.choices[0].delta.content:
#                         full_response += chunk.choices[0].delta.content
#                         message_placeholder.markdown(full_response + "â–Œ")
#                 message_placeholder.markdown(full_response)
                
#                 # [í† í° ì •ì‚° - ì‚¬ìš©ì ìš”ì²­ ê¸°ëŠ¥ ìœ ì§€]
#                 output_tokens_estimate = calc_tokens(full_response)
#                 st.session_state.total_tokens += (input_tokens_estimate + output_tokens_estimate)
                
#                 # ì‚¬ì´ë“œë°” ê°±ì‹ ì„ ìœ„í•´(ì„ íƒ) - ë„ˆë¬´ ê¹œë¹¡ì´ë©´ ì£¼ì„ ì²˜ë¦¬
#                 # st.rerun()
#             else:
#                 full_response = stream_response
#                 message_placeholder.error(full_response)
#         elif not stream_response and intent_result["type"] == "SEARCH" and not full_response:
#              # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ
#              pass

#     st.session_state.messages.append({"role": "assistant", "content": full_response})
import streamlit as st
import feedparser
import os
import urllib.parse
# [ë³€ê²½ë¨] ì „ë¬¸ í¬ë¡¤ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from newspaper import Article, Config
from dotenv import load_dotenv
from openai import OpenAI
import urllib3
import requests
from bs4 import BeautifulSoup


urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# --- ì„¤ì • ë° ì´ˆê¸°í™” ---
API_KEY = os.getenv("OPENAI_API_KEY")
API_BASE = os.getenv("OPENAI_API_BASE")
MODEL_NAME = os.getenv("LLM_MODEL_NAME", "gpt-3.5-turbo")

if API_BASE:
    client = OpenAI(api_key=API_KEY, base_url=API_BASE)
else:
    client = OpenAI(api_key=API_KEY)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="AI ë‰´ìŠ¤ íë ˆì´í„° (Pro)", page_icon="ğŸ“°", layout="wide")
st.title("ğŸ“° AI ì‹¬ì¸µ ë‰´ìŠ¤ ë¶„ì„ ì±—ë´‡")

# --- ì„¸ì…˜ ìƒíƒœ ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "total_tokens" not in st.session_state:
    st.session_state.total_tokens = 0

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("ğŸ“Š ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°")
    st.metric(label="ëˆ„ì  ì‚¬ìš© í† í°", value=f"{st.session_state.total_tokens:,} Tokens")
    if st.button("ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.session_state.total_tokens = 0
        st.rerun()

# --- í•µì‹¬ í•¨ìˆ˜ ---
def extract_real_url(entry):
    """
    Google News RSS entryì—ì„œ ì‹¤ì œ ì–¸ë¡ ì‚¬ ê¸°ì‚¬ URL ì¶”ì¶œ
    """
    try:
        if "summary" in entry:
            soup = BeautifulSoup(entry.summary, "html.parser")
            a = soup.find("a")
            if a and a.get("href"):
                return a["href"]
    except Exception:
        pass

    # fallback (ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë§í¬)
    return entry.link

def calc_tokens(text):
    if not text: return 0
    return int(len(text) * 1.2)

# [ì™„ì „ ë³€ê²½ë¨] newspaper3k ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•œ ê¹”ë”í•œ í¬ë¡¤ë§
# [ìˆ˜ì •ë¨] 1. requestsë¡œ ëš«ê³  -> 2. newspaperë¡œ ë¶„ì„í•˜ëŠ” 'í•˜ì´ë¸Œë¦¬ë“œ' ë°©ì‹
def fetch_article_content(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Referer": "https://www.google.com/"
        }

        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        config = Config()
        config.browser_user_agent = headers["User-Agent"]
        config.request_timeout = 10

        article = Article(url, language="ko", config=config)
        article.download(input_html=response.text)
        article.parse()

        text = article.text.strip()
        if len(text) < 300:
            return None

        return text[:2000] + "..."

    except Exception as e:
        print(f"[í¬ë¡¤ë§ ì‹¤íŒ¨] {url} / {e}")
        return None
    
def get_news_data(keyword):
    encoded_keyword = urllib.parse.quote(keyword)
    # [íŒ] ì •í™•ë„ë¥¼ ìœ„í•´ 'when:1d' (1ì¼ ì´ë‚´) ì˜µì…˜ì„ ì œê±°í•˜ê³  ì¼ë°˜ ê²€ìƒ‰ ì‚¬ìš©
    rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=ko&gl=KR&ceid=KR:ko"
    
    feed = feedparser.parse(rss_url)
    news_items = []
    
    if not feed.entries:
        return []

    for i, entry in enumerate(feed.entries[:3]):
        content = entry.get('summary', 'ìš”ì•½ ì—†ìŒ')
        source_type = "Google RSS ìš”ì•½"
        
        # ìƒìœ„ 2ê°œëŠ” ì „ë¬¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ê¸ì–´ì˜¤ê¸° ì‹œë„
        if i < 2:
            real_url = extract_real_url(entry)
            crawled_text = fetch_article_content(real_url)
            if crawled_text:
                content = crawled_text
                source_type = "ğŸŒ ì›¹ì‚¬ì´íŠ¸ ë³¸ë¬¸ (Newspaper3k)"
        
        news_items.append({
    "title": entry.title,
    "link": real_url,   # â† ì¤‘ìš”
    "content": content,
    "source_type": source_type
})

    
    return news_items

def classify_intent(user_input):
    system_prompt = "ì‚¬ìš©ìê°€ ë‰´ìŠ¤ ê²€ìƒ‰ì„ ì›í•˜ë©´ 'SEARCH:í‚¤ì›Œë“œ', ì•„ë‹ˆë©´ 'CHAT'ì´ë¼ê³  ë‹µí•˜ì„¸ìš”."
    input_tokens = calc_tokens(system_prompt + user_input)

    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": user_input}]
        )
        content = response.choices[0].message.content.strip()
        st.session_state.total_tokens += (input_tokens + calc_tokens(content))

        if content.startswith("SEARCH:"):
            return {"type": "SEARCH", "keyword": content.split("SEARCH:")[1].strip()}
        return {"type": "CHAT", "keyword": None}
    except:
        return {"type": "CHAT", "keyword": None}

def get_llm_response(prompt, context_type="general", news_data=None):
    messages = st.session_state.messages.copy()
    
    if context_type == "news_summary" and news_data:
        news_context = ""
        for idx, item in enumerate(news_data, 1):
            news_context += f"[{idx}] {item['title']} ({item['source_type']})\në§í¬: {item['link']}\në‚´ìš©: {item['content']}\n\n"
            
        system_prompt = "ë‹¹ì‹ ì€ ì‹¬ì¸µ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. [ë‰´ìŠ¤ ë°ì´í„°]ë¥¼ ë³´ê³  ë‹µë³€í•˜ì„¸ìš”. 'ì›¹ì‚¬ì´íŠ¸ ë³¸ë¬¸'ì´ ìˆë‹¤ë©´ ê·¸ ë‚´ìš©ì„ ì ê·¹ ì¸ìš©í•˜ì„¸ìš”."
        user_content = f"ì§ˆë¬¸: {prompt}\n\n[ë‰´ìŠ¤ ë°ì´í„°]\n{news_context}"
        
        messages_for_api = [{"role": "system", "content": system_prompt}] + messages
        messages_for_api.append({"role": "user", "content": user_content})
    else:
        messages_for_api = [{"role": "system", "content": "ë„ì›€ì´ ë˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤."}] + messages
        messages_for_api.append({"role": "user", "content": prompt})

    all_text = "".join([str(m["content"]) for m in messages_for_api])
    input_tokens = calc_tokens(all_text)

    try:
        stream = client.chat.completions.create(model=MODEL_NAME, messages=messages_for_api, stream=True)
        return stream, input_tokens
    except Exception as e:
        return f"ì—ëŸ¬: {e}", 0

# --- ë©”ì¸ ë¡œì§ ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        intent_result = classify_intent(prompt)
        stream_response = None
        input_tokens = 0
        
        if intent_result["type"] == "SEARCH":
            keyword = intent_result["keyword"]
            with st.status(f"ğŸ•µï¸ '{keyword}' ë¶„ì„ ì¤‘...", expanded=True) as status:
                news_items = get_news_data(keyword)
                
                # ì„±ê³µ ê°œìˆ˜ í™•ì¸
                success = sum(1 for item in news_items if "Newspaper3k" in item['source_type'])
                if success > 0:
                    status.update(label=f"âœ… {success}ê±´ ì›ë¬¸ ë¶„ì„ ì„±ê³µ!", state="complete", expanded=False)
                else:
                    status.update(label="âš ï¸ ì¼ë¶€ ë³´ì•ˆ ì ‘ì† ì‹¤íŒ¨ (ìš”ì•½ë³¸ ì‚¬ìš©)", state="complete", expanded=False)
                
                stream_response, input_tokens = get_llm_response(prompt, "news_summary", news_items)
        else:
            stream_response, input_tokens = get_llm_response(prompt, "general")

        if stream_response and not isinstance(stream_response, str):
            for chunk in stream_response:
                if chunk.choices[0].delta.content:
                    full_response += chunk.choices[0].delta.content
                    message_placeholder.markdown(full_response + "â–Œ")
            
            st.session_state.total_tokens += (input_tokens + calc_tokens(full_response))
            message_placeholder.markdown(full_response)
        else:
            message_placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})